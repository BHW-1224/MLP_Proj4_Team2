{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# STEP 2 – LightGBM (LGBMRegressor 기반, Kaggle-safe version)\n# Hull Tactical Market Prediction\n# ============================================================\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom lightgbm import LGBMRegressor\nimport kaggle_evaluation.default_inference_server\n\nTRAIN_PATH = \"/kaggle/input/hull-tactical-market-prediction/train.csv\"\nTEST_PATH  = \"/kaggle/input/hull-tactical-market-prediction/test.csv\"\nTARGET_NAME = \"market_forward_excess_returns\"\n\nIS_RERUN = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n\nmodel = None\nfeature_cols = []\nGLOBAL_ALPHA = 0.5\ndf_train = None\ny_full = None\n\nprint(\"Loading training data...\")\n\n# ------------------------------------------------------------\n# 1. 유틸 함수\n# ------------------------------------------------------------\n\ndef make_time_folds(df, n_folds=3, val_window=180):\n    dates = np.sort(df[\"date_id\"].unique())\n    n_dates = len(dates)\n\n    total_val = n_folds * val_window\n    if total_val >= n_dates:\n        val_window = max(30, n_dates // (n_folds + 1))\n        total_val = n_folds * val_window\n\n    start_val = n_dates - total_val\n    folds = []\n\n    for k in range(n_folds):\n        val_s = start_val + k * val_window\n        val_e = val_s + val_window\n        val_dates = dates[val_s:val_e]\n        tr_dates  = dates[:val_s]\n        folds.append((tr_dates, val_dates))\n\n    return folds\n\n\ndef pred_to_allocation(y_pred, ref_series, alpha):\n    mean = np.mean(ref_series)\n    std  = np.std(ref_series)\n    z = (y_pred - mean) / (std + 1e-9)\n    w = 1.0 + alpha * z\n    return np.clip(w, 0, 2)\n\n\ndef hull_like_metric(df_val, positions):\n    fwd = df_val[\"forward_returns\"].values\n    rf  = df_val[\"risk_free_rate\"].values\n    mkt = df_val[\"market_forward_excess_returns\"].values\n\n    strat = positions * fwd\n    excess = strat - rf\n\n    mu_s = np.mean(excess)\n    mu_m = np.mean(mkt)\n    vol_s = np.std(excess)\n    vol_m = np.std(mkt)\n\n    sharpe_s = mu_s / (vol_s + 1e-9)\n    sharpe_m = mu_m / (vol_m + 1e-9)\n\n    vol_ratio = vol_s / (vol_m + 1e-9)\n\n    score = sharpe_s\n    if vol_ratio > 1.2:\n        score -= (vol_ratio - 1.2)\n    if mu_s < mu_m:\n        score -= (mu_m - mu_s)\n\n    return score, vol_ratio, sharpe_s, sharpe_m, mu_s, mu_m\n\n\n# ------------------------------------------------------------\n# 2. 모델 학습\n# ------------------------------------------------------------\n\nif os.path.exists(TRAIN_PATH):\n    train_df = pd.read_csv(TRAIN_PATH).sort_values(\"date_id\")\n    df_train = train_df.copy()\n    y_full = train_df[TARGET_NAME].values\n\n    # test 샘플로 feature 교집합 추출\n    sample_test = pd.read_csv(TEST_PATH, nrows=10)\n    common_cols = list(set(train_df.columns) & set(sample_test.columns))\n    feature_cols = sorted([c for c in common_cols if c not in [\"date_id\", \"is_scored\"]])\n\n    print(\"Num features:\", len(feature_cols))\n\n    X_full = train_df[feature_cols]\n\n    # 모델 설정 (sklearn API)\n    base_model = LGBMRegressor(\n        n_estimators=600,\n        learning_rate=0.01,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=50,\n        random_state=42\n    )\n\n    # -------------------------\n    # CV + alpha tuning (local only)\n    # -------------------------\n    if not IS_RERUN:\n        print(\"Running CV for alpha tuning...\")\n        folds = make_time_folds(train_df, n_folds=2, val_window=150)\n        alpha_grid = [0.3, 0.4, 0.5, 0.6]\n\n        best_alphas = []\n\n        for fold_id, (tr_dates, va_dates) in enumerate(folds):\n            print(\"\\nFold\", fold_id)\n\n            tr_mask = train_df[\"date_id\"].isin(tr_dates)\n            va_mask = train_df[\"date_id\"].isin(va_dates)\n\n            X_tr, y_tr = X_full[tr_mask], y_full[tr_mask]\n            X_va, y_va = X_full[va_mask], y_full[va_mask]\n\n            # 학습\n            cv_model = LGBMRegressor(\n                n_estimators=600,\n                learning_rate=0.01,\n                num_leaves=64,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                min_child_samples=50,\n                random_state=42\n            )\n            cv_model.fit(X_tr, y_tr)\n\n            pred_va = cv_model.predict(X_va)\n            df_val = train_df[va_mask].copy()\n\n            best_score = -np.inf\n            best_alpha = 0.5\n\n            for a in alpha_grid:\n                alloc = pred_to_allocation(pred_va, y_full, a)\n                score, vr, sharpe_s, sharpe_m, mu_s, mu_m = hull_like_metric(df_val, alloc)\n\n                print(f\" alpha={a} score={score:.4f} vol_ratio={vr:.3f}\")\n\n                if vr <= 1.2 and score > best_score:\n                    best_score = score\n                    best_alpha = a\n\n            print(\" Best alpha:\", best_alpha)\n            best_alphas.append(best_alpha)\n\n        GLOBAL_ALPHA = float(np.median(best_alphas))\n        print(\"\\nSelected GLOBAL_ALPHA:\", GLOBAL_ALPHA)\n\n    else:\n        GLOBAL_ALPHA = 0.5\n        print(\"Kaggle rerun detected → alpha fixed to 0.5\")\n\n    # -------------------------\n    # 최종 모델 학습\n    # -------------------------\n    print(\"\\nTraining final model...\")\n\n    final_model = LGBMRegressor(\n        n_estimators=600,\n        learning_rate=0.01,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=50,\n        random_state=42\n    )\n\n    final_model.fit(X_full, y_full)\n    model = final_model\n\n    print(\"Final model ready.\")\n\nelse:\n    print(\"train.csv not found → fallback\")\n    model = None\n    feature_cols = []\n    GLOBAL_ALPHA = 0.5\n\n\n# ------------------------------------------------------------\n# 3. predict() — Kaggle 스트리밍 입력 처리 함수\n# ------------------------------------------------------------\n\ndef predict(test: pl.DataFrame) -> pl.DataFrame:\n    test_pd = test.to_pandas()\n\n    if model is None or len(feature_cols) == 0:\n        alloc = np.ones(len(test_pd), dtype=\"float32\")\n        return pl.DataFrame({\"date_id\": test[\"date_id\"], \"prediction\": alloc})\n\n    # feature 매칭\n    X_test = pd.DataFrame(index=test_pd.index)\n    for f in feature_cols:\n        if f in test_pd.columns:\n            X_test[f] = test_pd[f]\n        else:\n            X_test[f] = 0.0\n\n    X_test = X_test.fillna(0.0)\n\n    pred = model.predict(X_test)\n    alloc = pred_to_allocation(pred, y_full, GLOBAL_ALPHA).astype(\"float32\")\n\n    return pl.DataFrame({\n        \"date_id\": test[\"date_id\"],\n        \"prediction\": alloc\n    })\n\n\n# ------------------------------------------------------------\n# 4. 서버 실행 (수정 금지)\n# ------------------------------------------------------------\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\"/kaggle/input/hull-tactical-market-prediction/\", )\n    )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T10:21:35.390388Z","iopub.execute_input":"2025-11-19T10:21:35.391097Z","iopub.status.idle":"2025-11-19T10:22:01.920838Z","shell.execute_reply.started":"2025-11-19T10:21:35.391072Z","shell.execute_reply":"2025-11-19T10:22:01.919994Z"}},"outputs":[{"name":"stdout","text":"Loading training data...\nNum features: 94\nRunning CV for alpha tuning...\n\nFold 0\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004696 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 21576\n[LightGBM] [Info] Number of data points in the train set: 8721, number of used features: 94\n[LightGBM] [Info] Start training from score 0.000047\n alpha=0.3 score=0.0177 vol_ratio=1.030\n alpha=0.4 score=0.0169 vol_ratio=1.041\n alpha=0.5 score=0.0162 vol_ratio=1.053\n alpha=0.6 score=0.0154 vol_ratio=1.065\n Best alpha: 0.3\n\nFold 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004358 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 21575\n[LightGBM] [Info] Number of data points in the train set: 8871, number of used features: 94\n[LightGBM] [Info] Start training from score 0.000044\n alpha=0.3 score=0.0729 vol_ratio=1.056\n alpha=0.4 score=0.0698 vol_ratio=1.078\n alpha=0.5 score=0.0667 vol_ratio=1.101\n alpha=0.6 score=0.0638 vol_ratio=1.124\n Best alpha: 0.3\n\nSelected GLOBAL_ALPHA: 0.3\n\nTraining final model...\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004522 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 21580\n[LightGBM] [Info] Number of data points in the train set: 9021, number of used features: 94\n[LightGBM] [Info] Start training from score 0.000053\nFinal model ready.\n","output_type":"stream"}],"execution_count":19}]}